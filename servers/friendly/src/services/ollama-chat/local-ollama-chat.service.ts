/**
 * Local Ollama Chat ì„œë¹„ìŠ¤
 * ë¡œì»¬ Ollama ì„œë²„ì™€ í†µì‹  (ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
 */

import { Ollama } from 'ollama';
import { BaseOllamaChatService } from './base-ollama-chat.service';
import type {
  ChatMessage,
  ChatOptions,
  LocalOllamaChatConfig,
  BatchChatRequest,
  BatchChatResult,
  BatchOptions,
  BatchAskRequest,
} from './ollama-chat.types';

export class LocalOllamaChatService extends BaseOllamaChatService {
  private client: Ollama;

  constructor(config: LocalOllamaChatConfig) {
    super(config.model, config.timeout ?? 60000);
    this.client = new Ollama({
      host: config.url,
    });
  }

  /**
   * ì„œë²„ ìƒíƒœ í™•ì¸ (ì—°ê²° ê°€ëŠ¥ ì—¬ë¶€ë§Œ í™•ì¸)
   */
  async checkStatus(): Promise<boolean> {
    try {
      await this.client.list();
      console.log(`âœ… Local Ollama ì—°ê²°ë¨ (ëª¨ë¸: ${this.model})`);
      return true;
    } catch {
      console.error('âŒ Local Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨');
      console.error('ğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve');
      return false;
    }
  }

  /**
   * ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
   */
  async chat(messages: ChatMessage[], options?: ChatOptions): Promise<string> {
    // format ê¸°ë³¸ê°’: 'json' ('text' ëª…ì‹œ ì‹œ í…ìŠ¤íŠ¸ ì‘ë‹µ)
    const format = options?.format === 'text' ? undefined : 'json';

    try {
      const response = await this.client.chat({
        model: this.model,
        messages,
        stream: false,
        format,
        options: {
          temperature: options?.temperature,
          top_p: options?.top_p,
          num_ctx: options?.num_ctx ?? 2048,
          num_predict: options?.num_predict,
        },
      });

      return response.message?.content || '';
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`Local Chat ì˜¤ë¥˜: ${error.message}`);
      }
      throw error;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async listModels(): Promise<string[]> {
    try {
      const response = await this.client.list();
      return response.models?.map((m) => m.name) || [];
    } catch {
      return [];
    }
  }

  /**
   * ë°°ì¹˜ ì±„íŒ… (ìˆœì°¨ ì²˜ë¦¬)
   * @template T - ì‘ë‹µ íƒ€ì… (parseJson: true ì‹œ ì‚¬ìš©)
   */
  async chatBatch<T = string>(
    requests: BatchChatRequest[],
    options?: BatchOptions
  ): Promise<BatchChatResult<T>[]> {
    const results: BatchChatResult<T>[] = [];

    for (let i = 0; i < requests.length; i++) {
      const req = requests[i];
      try {
        const rawResponse = await this.chat(req.messages, req.options);

        // JSON íŒŒì‹± ì˜µì…˜ (ê¸°ë³¸: true)
        const shouldParse = options?.parseJson !== false;
        const response = shouldParse
          ? this.parseJsonResponse<T>(rawResponse) ?? (rawResponse as unknown as T)
          : (rawResponse as unknown as T);

        results.push({
          id: req.id,
          success: true,
          response,
        });
      } catch (error) {
        results.push({
          id: req.id,
          success: false,
          error: error instanceof Error ? error.message : String(error),
        });
      }
      options?.onProgress?.(i + 1, requests.length);
    }

    return results;
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ë°°ì¹˜ Ask (ìˆœì°¨ ì²˜ë¦¬)
   * @template T - ì‘ë‹µ íƒ€ì… (parseJson: true ì‹œ ì‚¬ìš©)
   */
  async askBatch<T = string>(
    systemPrompt: string,
    requests: BatchAskRequest[],
    options?: BatchOptions
  ): Promise<BatchChatResult<T>[]> {
    const chatRequests: BatchChatRequest[] = requests.map((req) => ({
      id: req.id,
      messages: [
        { role: 'system' as const, content: systemPrompt },
        { role: 'user' as const, content: req.userMessage },
      ],
      options: req.options,
    }));

    return this.chatBatch<T>(chatRequests, options);
  }
}
