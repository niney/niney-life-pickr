/**
 * Local Ollama Chat ì„œë¹„ìŠ¤
 * ë¡œì»¬ Ollama ì„œë²„ì™€ í†µì‹  (ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
 */

import { Ollama } from 'ollama';
import { BaseOllamaChatService } from './base-ollama-chat.service';
import type {
  ChatMessage,
  ChatOptions,
  LocalOllamaChatConfig,
  BatchChatRequest,
  BatchChatResult,
  BatchOptions,
} from './ollama-chat.types';

export class LocalOllamaChatService extends BaseOllamaChatService {
  private client: Ollama;

  constructor(config: LocalOllamaChatConfig) {
    super(config.model, config.timeout ?? 60000);
    this.client = new Ollama({
      host: config.url,
    });
  }

  /**
   * ì„œë²„ ìƒíƒœ í™•ì¸ (ì—°ê²° ê°€ëŠ¥ ì—¬ë¶€ë§Œ í™•ì¸)
   */
  async checkStatus(): Promise<boolean> {
    try {
      await this.client.list();
      console.log(`âœ… Local Ollama ì—°ê²°ë¨ (ëª¨ë¸: ${this.model})`);
      return true;
    } catch {
      console.error('âŒ Local Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨');
      console.error('ğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve');
      return false;
    }
  }

  /**
   * ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡
   */
  async chat(messages: ChatMessage[], options?: ChatOptions): Promise<string> {
    try {
      const response = await this.client.chat({
        model: this.model,
        messages,
        stream: false,
        format: options?.format,
        options: {
          temperature: options?.temperature,
          top_p: options?.top_p,
          num_ctx: options?.num_ctx ?? 2048,
          num_predict: options?.num_predict,
        },
      });

      return response.message?.content || '';
    } catch (error) {
      if (error instanceof Error) {
        throw new Error(`Local Chat ì˜¤ë¥˜: ${error.message}`);
      }
      throw error;
    }
  }

  /**
   * ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async listModels(): Promise<string[]> {
    try {
      const response = await this.client.list();
      return response.models?.map((m) => m.name) || [];
    } catch {
      return [];
    }
  }

  /**
   * ë°°ì¹˜ ì±„íŒ… (ìˆœì°¨ ì²˜ë¦¬)
   */
  async chatBatch(
    requests: BatchChatRequest[],
    options?: BatchOptions
  ): Promise<BatchChatResult[]> {
    const results: BatchChatResult[] = [];

    for (let i = 0; i < requests.length; i++) {
      const req = requests[i];
      try {
        const response = await this.chat(req.messages, req.options);
        results.push({
          id: req.id,
          success: true,
          response,
        });
      } catch (error) {
        results.push({
          id: req.id,
          success: false,
          error: error instanceof Error ? error.message : String(error),
        });
      }
      options?.onProgress?.(i + 1, requests.length);
    }

    return results;
  }
}
